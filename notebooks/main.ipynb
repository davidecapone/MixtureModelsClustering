{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable. ('_xsrf' argument missing from POST)."
     ]
    }
   ],
   "source": [
    "# Import used libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"image.cmap\"] = 'inferno'\n",
    "\n",
    "\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "                             angle, **kwargs))\n",
    "        \n",
    "def plot_gmm(gmm, X, label=True, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    labels = gmm.fit(X).predict(X)\n",
    "    if label:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "    for pos, covar, w in zip(gmm.means_, gmm.covars_, gmm.weights_):\n",
    "        draw_ellipse(pos, covar, alpha=w * w_factor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "![Clustering meme](./assets/meme.png \"Clustering meme\")\n",
    "\n",
    "## Non probabilistic models\n",
    "### K-means\n",
    "*K-means* is a centroid-based clustering algorithm. It's usually considered as the baseline for solving clustering problems.\n",
    "\n",
    "K-means has some big limitations, which are overcome by other clustering algorithms.\n",
    "\n",
    "We will see how this simple clustering algorithm works on some toy datasets, enlighting its main limitations.\n",
    "Then we will show how probabilistic models can overcome them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable. ('_xsrf' argument missing from POST)."
     ]
    }
   ],
   "source": [
    "# KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load data\n",
    "df_basic1 = pd.read_csv('./data/basic1.csv')\n",
    "df_basic2 = pd.read_csv('./data/basic2.csv')\n",
    "df_lines2 = pd.read_csv('./data/lines2.csv')\n",
    "df_basic4 = pd.read_csv('./data/basic4.csv')\n",
    "df_basic5 = pd.read_csv('./data/basic5.csv')\n",
    "\n",
    "# Apply KMeans\n",
    "kmeans_basic1 = KMeans(n_clusters=4, random_state=0).fit(df_basic1)\n",
    "kmeans_basic2 = KMeans(n_clusters=5, random_state=0).fit(df_basic2)\n",
    "kmeans_lines2 = KMeans(n_clusters=5, random_state=0).fit(df_lines2)\n",
    "kmeans_basic4 = KMeans(n_clusters=3, random_state=0).fit(df_basic4) \n",
    "kmeans_basic5 = KMeans(n_clusters=3, random_state=0).fit(df_basic5)\n",
    "\n",
    "# Plotting results\n",
    "fig, ax = plt.subplots(1, 5, figsize=(20, 5))\n",
    "ax[0].scatter(df_basic1['x'], df_basic1['y'], c=kmeans_basic1.labels_)\n",
    "ax[0].set_title('Basic1')\n",
    "ax[1].scatter(df_basic2['x'], df_basic2['y'], c=kmeans_basic2.labels_)\n",
    "ax[1].set_title('Basic2')\n",
    "ax[2].scatter(df_lines2['x'], df_lines2['y'], c=kmeans_lines2.labels_)\n",
    "ax[2].set_title('Lines2')\n",
    "ax[3].scatter(df_basic4['x'], df_basic4['y'], c=kmeans_basic4.labels_)\n",
    "ax[3].set_title('Basic4')\n",
    "ax[4].scatter(df_basic5['x'], df_basic5['y'], c=kmeans_basic5.labels_)\n",
    "ax[4].set_title('Basic5')\n",
    "fig.suptitle('KMeans')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans doesn't work well with non-circular clusters.\n",
    "\n",
    "Let's see how GMM works on the same data for comparison.\n",
    "First we try GMM with EM, using the same number of components as K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable. ('_xsrf' argument missing from POST)."
     ]
    }
   ],
   "source": [
    "# GMM with EM\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Apply Gaussian Mixture\n",
    "gmm_basic1 = GaussianMixture(n_components=4, random_state=0).fit(df_basic1)\n",
    "gmm_basic2 = GaussianMixture(n_components=5, random_state=0).fit(df_basic2)\n",
    "gmm_lines2 = GaussianMixture(n_components=5, random_state=0).fit(df_lines2)\n",
    "gmm_basic4 = GaussianMixture(n_components=3, random_state=0).fit(df_basic4)\n",
    "gmm_basic5 = GaussianMixture(n_components=3, random_state=0).fit(df_basic5)\n",
    "\n",
    "# Plotting results\n",
    "fig, ax = plt.subplots(1, 5, figsize=(20, 5))\n",
    "ax[0].scatter(df_basic1['x'], df_basic1['y'], c=gmm_basic1.predict(df_basic1))\n",
    "ax[0].set_title('Basic1')\n",
    "ax[1].scatter(df_basic2['x'], df_basic2['y'], c=gmm_basic2.predict(df_basic2))\n",
    "ax[1].set_title('Basic2')\n",
    "ax[2].scatter(df_lines2['x'], df_lines2['y'], c=gmm_lines2.predict(df_lines2))\n",
    "ax[2].set_title('Lines2')\n",
    "ax[3].scatter(df_basic4['x'], df_basic4['y'], c=gmm_basic4.predict(df_basic4))\n",
    "ax[3].set_title('Basic4')\n",
    "ax[4].scatter(df_basic5['x'], df_basic5['y'], c=gmm_basic5.predict(df_basic5))\n",
    "ax[4].set_title('Basic5')\n",
    "fig.suptitle('Gaussian Mixture Models with EM')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in this example, GMM with EM works better than K-means when the clusters are not in a circular shape, also it is still good with the convex ones. When the clusters are more line-shaped, GMM with EM still shows some limitations, this is probably due to the fact that GMM with EM is just a generalization of K-means, so it has the same limitations, even tho it is a bit more flexible.\n",
    "\n",
    "A drawback of both these techniques is that they require the number of clusters to be specified.\n",
    "\n",
    "Now let's see how Bayesian GMM works on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable. ('_xsrf' argument missing from POST)."
     ]
    }
   ],
   "source": [
    "# Bayesian GMM\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "# Apply Bayesian Gaussian Mixture\n",
    "## sklearn's implementation of Bayesian GMM requires the number of components to be specified, \n",
    "## but if the actual number of components is lower, the model will converge to the correct number of components\n",
    "bgmm_basic1 = BayesianGaussianMixture(n_components=20, random_state=0).fit(df_basic1)\n",
    "bgmm_basic2 = BayesianGaussianMixture(n_components=20, random_state=0).fit(df_basic2)\n",
    "bgmm_lines2 = BayesianGaussianMixture(n_components=20, random_state=0).fit(df_lines2)\n",
    "bgmm_basic4 = BayesianGaussianMixture(n_components=20, random_state=0).fit(df_basic4)\n",
    "bgmm_basic5 = BayesianGaussianMixture(n_components=20, random_state=0).fit(df_basic5)\n",
    "\n",
    "# Plotting results\n",
    "fig, ax = plt.subplots(1, 5, figsize=(20, 5))\n",
    "ax[0].scatter(df_basic1['x'], df_basic1['y'], c=bgmm_basic1.predict(df_basic1))\n",
    "ax[0].set_title('Basic1')\n",
    "ax[1].scatter(df_basic2['x'], df_basic2['y'], c=bgmm_basic2.predict(df_basic2))\n",
    "ax[1].set_title('Basic2')\n",
    "ax[2].scatter(df_lines2['x'], df_lines2['y'], c=bgmm_lines2.predict(df_lines2))\n",
    "ax[2].set_title('Lines2')\n",
    "ax[3].scatter(df_basic4['x'], df_basic4['y'], c=bgmm_basic4.predict(df_basic4))\n",
    "ax[3].set_title('Basic4')\n",
    "ax[4].scatter(df_basic5['x'], df_basic5['y'], c=bgmm_basic5.predict(df_basic5))\n",
    "ax[4].set_title('Basic5')\n",
    "fig.suptitle('Bayesian Gaussian Mixture Models')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian GMM is able to detect the number of clusters automatically, it works better than K-means in the case of non-spherical data, and seems to also detect line-shaped clusters, so it's better overall when compared to the two previous techniques."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "*Density-Based Special Clustering of Applications with Noise*, commonly know as DBSCAN, is another clustering algorithm. As the name suggests, it belongs to the family of density-based clustering algorithms. It doesn't require the number of clusters to be specified, as Bayesian GMM. Usually it works well with dense, clearly separated clusters, of any shape, even if the separation is not linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable. ('_xsrf' argument missing from POST)."
     ]
    }
   ],
   "source": [
    "# DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Load data\n",
    "df_dart2 = pd.read_csv('./data/dart2.csv')\n",
    "df_face = pd.read_csv('./data/face.csv')\n",
    "df_spiral2 = pd.read_csv('./data/spiral2.csv')\n",
    "df_spirals = pd.read_csv('./data/spirals.csv')\n",
    "df_un2 = pd.read_csv('./data/un2.csv')\n",
    "df_wave = pd.read_csv('./data/wave.csv')\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan_dart2 = DBSCAN(eps=6, min_samples=2).fit(df_dart2)\n",
    "dbscan_face = DBSCAN(eps=9, min_samples=4).fit(df_face)\n",
    "dbscan_spiral2 = DBSCAN(eps=5, min_samples=3).fit(df_spiral2)\n",
    "dbscan_spirals = DBSCAN(eps=7, min_samples=4).fit(df_spirals)\n",
    "dbscan_un2 = DBSCAN(eps=7, min_samples=5).fit(df_un2)\n",
    "dbscan_wave = DBSCAN(eps=6, min_samples=2).fit(df_wave)\n",
    "\n",
    "# Plotting results\n",
    "fig, ax = plt.subplots(1, 6, figsize=(20, 5))\n",
    "ax[0].scatter(df_dart2['x'], df_dart2['y'], c=dbscan_dart2.labels_)\n",
    "ax[0].set_title('Dart2')\n",
    "ax[1].scatter(df_face['x'], df_face['y'], c=dbscan_face.labels_)\n",
    "ax[1].set_title('Face')\n",
    "ax[2].scatter(df_spiral2['x'], df_spiral2['y'], c=dbscan_spiral2.labels_)\n",
    "ax[2].set_title('Spiral2')\n",
    "ax[3].scatter(df_spirals['x'], df_spirals['y'], c=dbscan_spirals.labels_)\n",
    "ax[3].set_title('Spirals')\n",
    "ax[4].scatter(df_un2['x'], df_un2['y'], c=dbscan_un2.labels_)\n",
    "ax[4].set_title('Un2')\n",
    "ax[5].scatter(df_wave['x'], df_wave['y'], c=dbscan_wave.labels_)\n",
    "ax[5].set_title('Wave')\n",
    "fig.suptitle('DBSCAN')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment confirms that DBSCAN works well with non-linearly separable data.\n",
    "\n",
    "The main flaw that has been found during these tests, is that the hyperparameters of DBSCAN are not easy to tune, and they can have a big impact on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable. ('_xsrf' argument missing from POST)."
     ]
    }
   ],
   "source": [
    "# GMM with EM\n",
    "\n",
    "# Apply GMM with EM\n",
    "gmm_dart2 = GaussianMixture(n_components=4, random_state=0).fit(df_dart2)\n",
    "gmm_face = GaussianMixture(n_components=4, random_state=0).fit(df_face)\n",
    "gmm_spiral2 = GaussianMixture(n_components=2, random_state=0).fit(df_spiral2)\n",
    "gmm_spirals = GaussianMixture(n_components=3, random_state=0).fit(df_spirals)\n",
    "gmm_un2 = GaussianMixture(n_components=3, random_state=0).fit(df_un2)\n",
    "gmm_wave = GaussianMixture(n_components=4, random_state=0).fit(df_wave)\n",
    "\n",
    "# Plotting results\n",
    "fig, ax = plt.subplots(1, 6, figsize=(20, 5))\n",
    "ax[0].scatter(df_dart2['x'], df_dart2['y'], c=gmm_dart2.predict(df_dart2))\n",
    "ax[0].set_title('Dart2')\n",
    "ax[1].scatter(df_face['x'], df_face['y'], c=gmm_face.predict(df_face))\n",
    "ax[1].set_title('Face')\n",
    "ax[2].scatter(df_spiral2['x'], df_spiral2['y'], c=gmm_spiral2.predict(df_spiral2))\n",
    "ax[2].set_title('Spiral2')\n",
    "ax[3].scatter(df_spirals['x'], df_spirals['y'], c=gmm_spirals.predict(df_spirals))\n",
    "ax[3].set_title('Spirals')\n",
    "ax[4].scatter(df_un2['x'], df_un2['y'], c=gmm_un2.predict(df_un2))\n",
    "ax[4].set_title('Un2')\n",
    "ax[5].scatter(df_wave['x'], df_wave['y'], c=gmm_wave.predict(df_wave))\n",
    "ax[5].set_title('Wave')\n",
    "fig.suptitle('Gaussian Mixture Models with EM')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN usually works better than K-means when the data is not linearly separable, this should be still valid when comparing DBSCAN vs GMM with EM, but this experiment showed that in some cases GMM with EM can still detect some degree of non-linearity, this could also be due to the fact that probabilistic models account for the uncertainty, so they can be more flexible to non-linearities than non-probabilistic models. Beware that these datasets are very simple, so this could not be true in more complex cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable. ('_xsrf' argument missing from POST)."
     ]
    }
   ],
   "source": [
    "# Bayesian GMM\n",
    "\n",
    "# Apply Bayesian Gaussian Mixture\n",
    "## sklearn's implementation of Bayesian GMM requires the number of components to be specified, \n",
    "## but if the actual number of components is lower, the model will converge to the correct number of components\n",
    "bgmm_dart2 = BayesianGaussianMixture(n_components=20, random_state=0).fit(df_dart2)\n",
    "bgmm_face = BayesianGaussianMixture(n_components=20, random_state=0).fit(df_face)\n",
    "bgmm_spiral2 = BayesianGaussianMixture(n_components=20, random_state=0).fit(df_spiral2)\n",
    "bgmm_spirals = BayesianGaussianMixture(n_components=20, random_state=0).fit(df_spirals)\n",
    "bgmm_un2 = BayesianGaussianMixture(n_components=20, random_state=0).fit(df_un2)\n",
    "bgmm_wave = BayesianGaussianMixture(n_components=20, random_state=0).fit(df_wave)\n",
    "\n",
    "# Plotting results\n",
    "fig, ax = plt.subplots(1, 6, figsize=(20, 5))\n",
    "ax[0].scatter(df_dart2['x'], df_dart2['y'], c=bgmm_dart2.predict(df_dart2))\n",
    "ax[0].set_title('Dart2')\n",
    "ax[1].scatter(df_face['x'], df_face['y'], c=bgmm_face.predict(df_face))\n",
    "ax[1].set_title('Face')\n",
    "ax[2].scatter(df_spiral2['x'], df_spiral2['y'], c=bgmm_spiral2.predict(df_spiral2))\n",
    "ax[2].set_title('Spiral2')\n",
    "ax[3].scatter(df_spirals['x'], df_spirals['y'], c=bgmm_spirals.predict(df_spirals))\n",
    "ax[3].set_title('Spirals')\n",
    "ax[4].scatter(df_un2['x'], df_un2['y'], c=bgmm_un2.predict(df_un2))\n",
    "ax[4].set_title('Un2')\n",
    "ax[5].scatter(df_wave['x'], df_wave['y'], c=bgmm_wave.predict(df_wave))\n",
    "ax[5].set_title('Wave')\n",
    "fig.suptitle('Bayesian Gaussian Mixture Models')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of non-linearly separable data BGMM showed that it isn't able to infer autonomously the actual number of components, in the following this will be forced as in the case of GMM with EM and K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable. ('_xsrf' argument missing from POST)."
     ]
    }
   ],
   "source": [
    "# Bayesian GMM\n",
    "\n",
    "# Apply Bayesian Gaussian Mixture\n",
    "## sklearn's implementation of Bayesian GMM requires the number of components to be specified, \n",
    "## but if the actual number of components is lower, the model will converge to the correct number of components\n",
    "bgmm_dart2 = BayesianGaussianMixture(n_components=4, random_state=0).fit(df_dart2)\n",
    "bgmm_face = BayesianGaussianMixture(n_components=4, random_state=0).fit(df_face)\n",
    "bgmm_spiral2 = BayesianGaussianMixture(n_components=2, random_state=0).fit(df_spiral2)\n",
    "bgmm_spirals = BayesianGaussianMixture(n_components=3, random_state=0).fit(df_spirals)\n",
    "bgmm_un2 = BayesianGaussianMixture(n_components=3, random_state=0).fit(df_un2)\n",
    "bgmm_wave = BayesianGaussianMixture(n_components=4, random_state=0).fit(df_wave)\n",
    "\n",
    "# Plotting results\n",
    "fig, ax = plt.subplots(1, 6, figsize=(20, 5))\n",
    "ax[0].scatter(df_dart2['x'], df_dart2['y'], c=bgmm_dart2.predict(df_dart2))\n",
    "ax[0].set_title('Dart2')\n",
    "ax[1].scatter(df_face['x'], df_face['y'], c=bgmm_face.predict(df_face))\n",
    "ax[1].set_title('Face')\n",
    "ax[2].scatter(df_spiral2['x'], df_spiral2['y'], c=bgmm_spiral2.predict(df_spiral2))\n",
    "ax[2].set_title('Spiral2')\n",
    "ax[3].scatter(df_spirals['x'], df_spirals['y'], c=bgmm_spirals.predict(df_spirals))\n",
    "ax[3].set_title('Spirals')\n",
    "ax[4].scatter(df_un2['x'], df_un2['y'], c=bgmm_un2.predict(df_un2))\n",
    "ax[4].set_title('Un2')\n",
    "ax[5].scatter(df_wave['x'], df_wave['y'], c=bgmm_wave.predict(df_wave))\n",
    "ax[5].set_title('Wave')\n",
    "fig.suptitle('Bayesian Gaussian Mixture Models\\nwith Correct Number of Components', y=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gmm(bgmm_spirals, df_spirals['x'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even BGMM has the same problem of GMM with EM and K-means when the clusters are non-linearly separable, but in a limited way. It is able to detect a higher degree of non-linearity than GMM with EM. A problem of this solution is that the number of components needs to be specified, so one of the main advantages of Bayesian GMM is lost."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Hierarchical clustering\n",
    "*Hierarchical clustering* is a method for clustering. It creates groups so that objects within a group are similar to each other and different from objects that belong to another group. \n",
    "Different approches to solve the Hierarchical problem include Agglomerative (bottom-up) and Divisive (top-down).\n",
    "## Probabilistic models\n",
    "### Bayesian Gaussian Mixture Models\n",
    "### Bayesian Mixture Models with other distributions\n",
    "### Mixture Models with EM\n",
    "### Model comparisons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
